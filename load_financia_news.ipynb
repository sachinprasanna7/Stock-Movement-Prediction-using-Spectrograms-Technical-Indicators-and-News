{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATMs to become virtual bank branches, accept d...</td>\n",
       "      <td>May 26, 2020, Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDFC First Bank seniors to forgo 65% of bonus ...</td>\n",
       "      <td>May 26, 2020, Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huge scam in YES Bank for many years, says Enf...</td>\n",
       "      <td>May 25, 2020, Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank of Maharashtra sanctioned Rs 2,789 cr in ...</td>\n",
       "      <td>May 24, 2020, Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DCB Bank's profit before tax declines 37.6% to...</td>\n",
       "      <td>May 23, 2020, Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                    Date\n",
       "0  ATMs to become virtual bank branches, accept d...   May 26, 2020, Tuesday\n",
       "1  IDFC First Bank seniors to forgo 65% of bonus ...   May 26, 2020, Tuesday\n",
       "2  Huge scam in YES Bank for many years, says Enf...    May 25, 2020, Monday\n",
       "3  Bank of Maharashtra sanctioned Rs 2,789 cr in ...    May 24, 2020, Sunday\n",
       "4  DCB Bank's profit before tax declines 37.6% to...  May 23, 2020, Saturday"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('data/news/dataset.csv')\n",
    "\n",
    "\n",
    "#keep only the columns we need - title and date\n",
    "df = df[['Title', 'Date']]\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4726, 2)\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Filter the DataFrame for dates between 2018 and 2020\n",
    "filtered_df = df[(df['Date'] >= '2018-01-01') & (df['Date'] <= '2020-12-31')]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "filtered_df.head()\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('filtered_dataset.csv', index=False)\n",
    "\n",
    "# Display the shape of the filtered DataFrame\n",
    "print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative', 'score': 0.9966173768043518}, {'label': 'Positive', 'score': 1.0}, {'label': 'Negative', 'score': 0.9999710321426392}, {'label': 'Neutral', 'score': 0.9889442920684814}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "results = nlp(sentences)\n",
    "print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubham\\AppData\\Local\\Temp\\ipykernel_16176\\3659432128.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Sentiment'] = filtered_df['Title'].apply(lambda x: nlp(x)[0]['label'])\n",
      "C:\\Users\\shubham\\AppData\\Local\\Temp\\ipykernel_16176\\3659432128.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['Sentiment_Score'] = filtered_df['Title'].apply(lambda x: nlp(x)[0]['score'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATMs to become virtual bank branches, accept d...</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDFC First Bank seniors to forgo 65% of bonus ...</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.951466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huge scam in YES Bank for many years, says Enf...</td>\n",
       "      <td>2020-05-25</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.996678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank of Maharashtra sanctioned Rs 2,789 cr in ...</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.995092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DCB Bank's profit before tax declines 37.6% to...</td>\n",
       "      <td>2020-05-23</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title       Date Sentiment  \\\n",
       "0  ATMs to become virtual bank branches, accept d... 2020-05-26   Neutral   \n",
       "1  IDFC First Bank seniors to forgo 65% of bonus ... 2020-05-26   Neutral   \n",
       "2  Huge scam in YES Bank for many years, says Enf... 2020-05-25   Neutral   \n",
       "3  Bank of Maharashtra sanctioned Rs 2,789 cr in ... 2020-05-24   Neutral   \n",
       "4  DCB Bank's profit before tax declines 37.6% to... 2020-05-23  Negative   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0         0.999943  \n",
       "1         0.951466  \n",
       "2         0.996678  \n",
       "3         0.995092  \n",
       "4         0.999972  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment for each news title in the filtered DataFrame\n",
    "# filtered_df['Sentiment'] = filtered_df['Title'].apply(lambda x: nlp(x)[0]['label'])\n",
    "# filtered_df['Sentiment_Score'] = filtered_df['Title'].apply(lambda x: nlp(x)[0]['score'])\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('filtered_dataset_with_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = 'your-api-key-here'\n",
    "\n",
    "# Define a function to get a response from OpenAI API\n",
    "def get_openai_response(prompt):\n",
    "    \"\"\"\n",
    "    Generates a response from OpenAI's GPT-3 model based on the provided prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text prompt to generate a response for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response text from the model.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Loop over the top 10 headlines in the DataFrame\n",
    "for headline in df['Title'].head(10):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following features from the news headline:\n",
    "    1. Named Entities\n",
    "    2. Topic\n",
    "    3. Sector/Industry\n",
    "    4. Financial Metrics\n",
    "    5. Tone\n",
    "\n",
    "    Headline: \"{headline}\"\n",
    "    \"\"\"\n",
    "    response = get_openai_response(prompt)\n",
    "    print(f\"Headline: {headline}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **News** | **Sentiment** | **Named Entities** | **Topic** | **Sector/Industry** | **Financial Metrics** | **Tone** |\n",
    "|-------------------------------------------|----------------|-----------------------------|-------------------------|------------------------|--------------------------|------------------|\n",
    "| NBFC yields yet to show contraction...    | Neutral        | NBFC, Govt                  | Spreads                  | Finance                 | None                     | Neutral          |\n",
    "| Sitharaman gives liquidity boost...       | Positive       | Sitharaman, Shadow Banks, MFIs | Liquidity Boost         | Finance                 | Rs 75,000 cr             | Optimistic       |\n",
    "| Kotak Bank profit before tax slips...     | Negative       | Kotak Bank                  | Earnings Report          | Banking                 | 10.6% decrease, Rs 2,674 cr | Cautionary    |\n",
    "| Differentiated bank holdco norms...       | Positive       | Bandhan, IDFC First          | Regulation Change        | Banking                 | None                     | Optimistic       |\n",
    "| Bad bank may start with Rs 60K-crore...   | Neutral        | Bad Bank, Govt               | NPA Management           | Banking                 | Rs 60K-crore NPAs         | Neutral          |\n",
    "| Some MSMEs may need deep restructuring... | Negative       | MSMEs, Union Bank            | Restructuring            | Finance                 | None                     | Cautionary       |\n",
    "| IBA considering proposal to set up...     | Positive       | IBA, PSBs                   | NPA Management           | Banking                 | None                     | Optimistic       |\n",
    "| Not offering any emergency loan...        | Neutral        | SBI, YONO                   | Clarification            | Banking                 | None                     | Neutral          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After filtering merge the news for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATMs to become virtual bank branches, accept d...</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDFC First Bank seniors to forgo 65% of bonus ...</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.951466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huge scam in YES Bank for many years, says Enf...</td>\n",
       "      <td>2020-05-25</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.996678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank of Maharashtra sanctioned Rs 2,789 cr in ...</td>\n",
       "      <td>2020-05-24</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.995092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DCB Bank's profit before tax declines 37.6% to...</td>\n",
       "      <td>2020-05-23</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.999972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Date Sentiment  \\\n",
       "0  ATMs to become virtual bank branches, accept d...  2020-05-26   Neutral   \n",
       "1  IDFC First Bank seniors to forgo 65% of bonus ...  2020-05-26   Neutral   \n",
       "2  Huge scam in YES Bank for many years, says Enf...  2020-05-25   Neutral   \n",
       "3  Bank of Maharashtra sanctioned Rs 2,789 cr in ...  2020-05-24   Neutral   \n",
       "4  DCB Bank's profit before tax declines 37.6% to...  2020-05-23  Negative   \n",
       "\n",
       "   Sentiment_Score  \n",
       "0         0.999943  \n",
       "1         0.951466  \n",
       "2         0.996678  \n",
       "3         0.995092  \n",
       "4         0.999972  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('data/news/sentiment.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming df is already defined and contains the necessary data\n",
    "# Group by 'Date' and aggregate the sentiments\n",
    "grouped_df = df.groupby('Date')['Sentiment'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "\n",
    "# Function to determine the most common sentiment for a given list of sentiments\n",
    "def most_common_sentiment(sentiments):\n",
    "    return Counter(sentiments.split(', ')).most_common(1)[0][0]\n",
    "\n",
    "# Apply the function to the 'Sentiment' column\n",
    "grouped_df['Daily_Sentiment'] = grouped_df['Sentiment'].apply(most_common_sentiment)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "grouped_df.head(20)\n",
    "\n",
    "# Drop the sentiment column\n",
    "grouped_df = grouped_df.drop(columns=['Sentiment'])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "grouped_df.to_csv('daily_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-01-21', '2018-01-28', '2018-01-29', '2018-02-02',\n",
      "               '2018-02-11', '2018-04-29', '2018-06-14', '2018-07-14',\n",
      "               '2018-08-12', '2018-08-19', '2018-10-27', '2018-12-01',\n",
      "               '2019-01-20', '2019-02-16', '2019-02-23', '2019-02-28',\n",
      "               '2019-03-19', '2019-03-21', '2019-04-07', '2019-04-15',\n",
      "               '2019-05-12', '2019-05-18', '2019-06-16', '2019-06-23',\n",
      "               '2019-06-26', '2019-07-21', '2019-07-26', '2019-07-27',\n",
      "               '2019-10-13', '2019-11-10', '2019-11-23', '2020-02-02',\n",
      "               '2020-04-25', '2020-05-16'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m missing_dates_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: missing_dates, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaily_Sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m: most_common_sentiment})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Concatenate the missing dates DataFrame with the grouped DataFrame\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m updated_grouped_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrouped_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dates_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the updated DataFrame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m updated_grouped_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:6955\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   6953\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 6955\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\n\u001b[0;32m   6957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6958\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\sorting.py:483\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    481\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    482\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 483\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    485\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "# Find the missing dates in the DataFrame\n",
    "missing_dates = pd.date_range(start=grouped_df['Date'].min(), end=grouped_df['Date'].max()).difference(grouped_df['Date'])\n",
    "print(missing_dates)\n",
    "\n",
    "# Create a DataFrame for the missing dates with the most common sentiment\n",
    "most_common_sentiment = grouped_df['Daily_Sentiment'].mode()[0]\n",
    "missing_dates_df = pd.DataFrame({'Date': missing_dates, 'Daily_Sentiment': most_common_sentiment})\n",
    "\n",
    "# Concatenate the missing dates DataFrame with the grouped DataFrame\n",
    "updated_grouped_df = pd.concat([grouped_df, missing_dates_df]).sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "updated_grouped_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Concatenate the missing dates DataFrame with the grouped DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m updated_grouped_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrouped_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dates_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the updated DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m updated_grouped_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:6955\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   6953\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 6955\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\n\u001b[0;32m   6957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6958\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\sorting.py:483\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    481\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    482\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 483\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    485\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "# Concatenate the missing dates DataFrame with the grouped DataFrame\n",
    "updated_grouped_df = pd.concat([grouped_df, missing_dates_df]).sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "updated_grouped_df.head(20)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "# updated_grouped_df.to_csv('updated_daily_sentiment.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
